<p>This weekend I experimented with some audio classification tools. It was an up and down experience.</p>

<p>I’m interested in a couple features – hotword detection ala “Hey Siri”, “Alexa”; sound event detection (i.e. identify a glass break or gunshot); and acoustic scene classification. I didn’t dig into general speech reco, I’ve dabbled with that in the past.</p>

<p>I experimented with two projects this weekend – the <a href="https://github.com/Kitt-AI/snowboy">Kitt.AI Snowboy hotword detection tool</a> and the <a href="https://github.com/TUT-ARG/DCASE2016-baseline-system-python">DCASE 2016 baseline system</a>. I spun up a single docker container that hosted both projects. This was a bit of a PITA, mostly due to getting sound devices to show up in a container. I should post something separate just on that adventure.</p>

<p>Ultimately I got them both working. The Snowboy detector works reasonably well with their universal model; the personal models you can create work also, tho they are not speaker independent. The DCASE code also spins up and training can be done on a standalone machine in a modest amount of time. Unfortunately, both these projects have very restrictive licenses, which makes them kind of useless for anything besides a weekend project.</p>

<p>At the root of almost all these systems is a common feature extraction algorithm, MFCC extraction. MFCCs are explained reasonably well <a href="http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">here</a> and the author provides a <a href="https://github.com/jameslyons/python_speech_features">python reference implementation</a> with an MIT license. I’m inclined to dig more into this path going forward.</p>
